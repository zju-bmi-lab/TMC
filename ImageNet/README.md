## Note
Note that considering the high training cost required to train QKFormer from scratch, we load the trained model of the initial QKformer and fine-tune it using our proposed loss function for 10 epochs. The pre-trained QKformer model can be load at [1](https://pan.baidu.com/s/1gRAZR9gkMr5ScHK-kwZAnw).


## Code Reference
Our code is developed based on the code from [Zhou, C., Zhang, H., Zhou, Z., Yu, L., Huang, L., Fan, X., Yuan, L., Ma, Z., Zhou, H., and Tian, Y. QKFormer: Hierarchical spiking transformer using qk attention. Advances in Neural Information Processing Systems, 2024.].